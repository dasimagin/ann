{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "show": true
   },
   "source": [
    "# L4 – Word embedding\n",
    "\n",
    "### Resources\n",
    "1. Possible [truncated svd](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD) implementation of sklearn.\n",
    "2. Tensorflow [tutorial](https://www.tensorflow.org/tutorials/word2vec) for word2vec and [implementation](https://github.com/tensorflow/models/tree/master/tutorials/embedding).\n",
    "3. GloVe [site](https://nlp.stanford.edu/projects/glove/).\n",
    "4. FastText [site](https://fasttext.cc).\n",
    "5. [Gensim](https://radimrehurek.com/gensim/) library.\n",
    "\n",
    "**Word embedding** – the collective name for a set of language modeling and feature learning techniques in natural language processing where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much higher dimensionality. Word and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing, sentiment analysis and translation. Usually, problem reduces to unsupervised learning on a large corpus of text. So, learning process is based on the idea that context of word closely associated with it. For example, the context can be a document in which the word is located or adjacent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Data\n",
    "\n",
    "As text corpus you may use simple english [wikipedia](https://simple.wikipedia.org/wiki/Simple_English_Wikipedia).\n",
    "1. 131K articles\n",
    "2. 2K common English words.\n",
    "\n",
    "Also you can use just english [wikipedia](https://en.wikipedia.org/wiki/Wikipedia):\n",
    "1. Includes 5М+ articles\n",
    "2. 2M+ different tokens\n",
    "\n",
    "#### Exercises\n",
    "1. Download all wikipedia articles.\n",
    "2. Make all preprocessing work that you need (e.g. remove markup)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 2. LSA (Latent semantic analysis)\n",
    "This solution uses full document as a context of word. So, we have some vocabulary $W$ and a set of documents $D$. Matrix $X$ with shape $|W| \\times |D|$ at position $w, d$ stores importance of word $w$ for document $d$. If word $w$ is not found in the document $d$ than at appropriate position $X$ has 0 (obviously, matrix is sparse).\n",
    "\n",
    "For each matrix you can find [SVD decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "$$X = U \\Sigma V^{T} \\text{, где }$$\n",
    "* $U$ – orthogonal matrix $|W| \\times |W|$ of left singular vectors\n",
    "* $\\Sigma$ – diagonal matrix $|W| \\times |D|$ of singular values\n",
    "* $V$ – orthogonal matrix $|D| \\times |D|$  of right singular vectors\n",
    "\n",
    "Let's suppouse that row $w$ in matrix $U\\Sigma$ is a vector that represents word $w$, and row $d$ of $V$ coresponds to document $d$. In some sense we already found the embeddings of words and documents at the same time. But size of vectors are determined by documents number $|D|$.\n",
    "\n",
    "Nevertheless you can use truncated SVD instead\n",
    "$$ X \\approx X_k = U_k \\Sigma_k V^{T}_k \\text{, where }$$\n",
    "* $U_k$ – $k$ left singular vectors\n",
    "* $\\Sigma_k$ – diagonal matrix $|k| \\times |k|$ of largest singular values\n",
    "* $V_k$ – $k$ right singular vectors\n",
    "\n",
    "Also it known that $X_k$ is the best approximation of $X$ in the term of [Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm) for all matrices of rank $k$.\n",
    "\n",
    "So, it is possible to obtain a compressed words' representations of size $k \\ll |W|$ as rows of matrix $U_k \\Sigma_k $. It just remains to understand how to calculate the original values of the matrix $X$. There is a set of approaches, here are some of them\n",
    "1. Binary flag that document contains the word.\n",
    "2. The number occurrences of word in document.\n",
    "3. Word frequency that is better known as $tf$ (it's possible also use $\\ln(1 + tf)$).\n",
    "4. [$tf \\cdot idf$](https://ru.wikipedia.org/wiki/TF-IDF).\n",
    "\n",
    "#### Further readning\n",
    "1. You also can read this [aticle](https://en.wikipedia.org/wiki/Latent_semantic_analysis).\n",
    "2. By the way, some [modification](http://www.netflixprize.com/assets/GrandPrize2009_BPC_BellKor.pdf) of SVD decompostion won at [Netflix prize](https://en.wikipedia.org/wiki/Netflix_Prize).\n",
    "3. It is easy to see that the problem of SVD decomposition is reduced to finding eigenvectors of matrix $X \\cdot X^T$.\n",
    "4. You can find [here](https://arxiv.org/abs/0909.4061) probabilistic algorithms of matrix decomposition.\n",
    "\n",
    "#### Exercises\n",
    "1. Find matrix $X$, using you favourite approach (maybe you need the sparse matrix class of scipy library).\n",
    "2. Find word embeddings of size $k = 128$.\n",
    "3. Implement k-nearest neighbor search for Euclidean norm.\n",
    "4. Find 10 nearest words for the set of words: 'cat', 'loves', 'clever' and 'mandarin'.\n",
    "5. Repeat experiment for cosinus as metric. What are the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. word2vec\n",
    "Let's consider perhaps the most popular model of word embeddings. This is mainly due to the fact that the received vectors have interesting properties. In particular, semantically similar words have close vectors, moreover linear operation of subtraction has meaning in the resulting vector space! The context of word at this time is a window of size $2c+1$, where interested word is located in the middle.\n",
    "\n",
    "#### Continuous bag-of-words\n",
    "The core idea is very simple. There is some very long text\n",
    "$$w_1, w_2, \\dots, w_T.$$\n",
    "Consider some word $w_t$ and its context in radius $c$, namely words\n",
    "$$w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots w_{t+c}.$$\n",
    "Intuition tells us that the context often really well defines the word in the middle. Then let our model restore the central word if its context is known\n",
    "$$\\mathcal{L} = - \\frac{1}{T}\\sum_{t} \\log \\mathbb{P}[w_t|w_{t-c}, \\dots, w_{t-1}, w_{t+1}, \\dots w_{t+c}].$$\n",
    "\n",
    "For each word $w_t$ there is some vector $v_t$, but if word $w_{t+i}$ is some context than we use vector $v'_{t+i}$. In this case we can represent context of word as the following sum\n",
    "$$s'_t = \\sum_{c \\leq i \\leq c, i \\neq 0} v'_{t+i}$$\n",
    "\n",
    "And define probabilty with softmax\n",
    "$$\\mathbb{P}[w_t|w_{t-c}, \\dots, w_{t-1}, w_{t-1}, \\dots w_{t+c}] = \\frac{ \\exp {s'}_t^T \\cdot v_t}{\\sum_j \\exp {s'}_t^T \\cdot v_j}.$$\n",
    "\n",
    "Note that this model is easy to present as 2-layer neural network:\n",
    "* The input is a sparse vector of dimension $|W|$ which at position $w'$ has one if word $w'$ is in context.\n",
    "* Further, we have matrix $V'$ of vectors $v'$.\n",
    "* Further, without any non-linearity matrix $V$ of vector $v$.\n",
    "* And then the softmax layer.\n",
    "\n",
    "#### Skip-gram\n",
    "Another model predicts context for some word. The design is approximately the same, we need to optimize\n",
    "$$\\mathcal{L} = -\\frac{1}{T} \\sum_t \\sum_{-c \\leq i \\leq c, i \\neq 0} \\mathbb{P}[w_{t+i}|w_t].$$\n",
    "\n",
    "The probability is approximated by the following model\n",
    "$$\\mathbb{P}[w_{t+i}|w_t] = \\frac{ \\exp {v'}_{t+i}^T \\cdot v_t}{\\sum_j \\exp {v'}_{j}^T \\cdot v_t}.$$\n",
    "\n",
    "\n",
    "#### Hierarchical softmax\n",
    "Creation of this mode was guided to reduce the complexity of learning. However, computing of sofmax's denominator is really expensive operation. Therefore, in practice, a few other models are used.\n",
    "\n",
    "This method was viewed in the seminar. Detailed information can be found [here](http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf) and practical using [here](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).    Briefly recall, for each word from the dictionary we define [Huffman code](https://en.wikipedia.org/wiki/Huffman_coding). More frequency words have shorter code. Then we construct the Huffman tree. In accordance with its code words are stored in leaves. In the non-leaf vertices are stored special hidden representations $h'$. \n",
    "\n",
    "$$\\mathbb{P}[w_{t+i}|w_t] = \\prod_{{h'}_{t+i}, r} \\sigma\\Big( \\mathcal{r} \\cdot {h'}^T_{t+i} v_t \\Big) \\text{, where}$$\n",
    "* ${h'}_{t+i}$ - hidden vector of word's path $w_{t+i}$\n",
    "* $\\mathcal{r}$ - equals $-1$, if we turn left and $+1$ in another case\n",
    "* $\\sigma$ - sigmoid function\n",
    "\n",
    "Using hierarchical softmax as approximation of softmax allows us to significantly reduce complexity of model training. \n",
    "\n",
    "#### Exercise\n",
    "1. How can you implement skip-gram model as neural network.\n",
    "2. Estimate complexity of one iteration of training the skip-gram model\n",
    "  * $T$ - text size\n",
    "  * $W$ - vocabulary size\n",
    "  * $c$ - context radius\n",
    "  * $d$ - embedding size\n",
    "3. Estimate complexity of using hierarchical softmax in the skip-gram model.\n",
    "\n",
    "#### Why sampling?\n",
    "Let see at softmax funciton\n",
    "$$\\mathbb{P}[x_i] = \\frac{\\exp f(x_i|\\theta)}{\\sum_j \\exp f(x_j|\\theta)}$$\n",
    "Optimizing this function is equivalent to optimizing its logarithm\n",
    "$$\\log \\frac{\\exp f(x_i|\\theta)}{\\sum_j \\exp f(x_j|\\theta)} = f(x_i|\\theta) - log \\sum_j \\exp f(x_j|\\theta)$$\n",
    "Than gradient $\\triangledown_{\\theta} \\mathbb{P}[x_i]$ is equal to\n",
    "$$\\triangledown_{\\theta} f(x_i|\\theta) - \\sum_k \\frac{\\exp f(x_k|\\theta)}{\\sum_j \\exp f(w_j|\\theta)} \\triangledown_{\\theta} f(x_k|\\theta)$$\n",
    "Inside the sum we can find $\\mathbb{P}[x_k]$ again. So, the previous expression can be written as\n",
    "$$\n",
    "\\triangledown_{\\theta} f(x_i|\\theta) - \\sum_k \\mathbb{P}[x_k] \\cdot \\triangledown_{\\theta} f(x_k|\\theta) = \n",
    "\\triangledown_{\\theta} f(x_i|\\theta) - \\mathbb{E}_{x \\sim \\mathbb{P}[x]} \\triangledown_{\\theta} f(x|\\theta)\n",
    "$$\n",
    "If we can sample $x \\sim \\mathbb{P}[x]$, then we has no sense to calculate the softmax itself. We just can train model, computing only gradient. It is possible due to the fact that we do not need the probability itself, but want to find representation of words only.\n",
    "\n",
    "#### Negative Sampling\n",
    "As the result, in practice, softmax loss functions is approximated with negative sampling, which looks like a series of logistic regressions\n",
    "$$\\mathcal{L} = - \\frac{1}{T}\\sum_{w_t}\n",
    "    \\sum_{-c \\leq i \\leq c, i \\neq 0} \\Big(\n",
    "        \\mathbb{P}[y = 1 | w_{t+i}, w_t] -\n",
    "        k \\cdot \\mathbb{E}_{\\tilde{w}_{tj} \\sim \\mathbb{P}[w_j|w_t]} \\mathbb{P}[y = 0 | \\tilde{w}_{tj}, w_t]\n",
    "    \\Big)\n",
    "$$\n",
    "where $y=1$, if  word is a context of word and $y=0$ in another case. А $\\tilde{w}_{tj}$ – sampling of words that don't belong to the context. Then main trick is how we will sample words $\\mathbb{P}[y | w_{t+i}, w_t]$. In paper [noise contrastive estimation](https://www.cs.helsinki.fi/u/ahyvarin/papers/Gutmann10AISTATS.pdf) some interesting approaches is suggested. But we are going to use more simple variation\n",
    "$$\\mathcal{L} = - \\frac{1}{T}\\sum_{w_t} \\Big(\n",
    "    \\sum_{-c \\leq i \\leq c, i \\neq 0} \\big(\n",
    "        \\sigma({v'}_{t+i}^T v_t) +\n",
    "        k \\cdot \\mathbb{E}_{w_j \\sim \\mathbb{Q}(w_j)} \\sigma (-{v'}_{j}^T v_t)\n",
    "    \\big)\n",
    "\\Big),\n",
    "$$\n",
    "to find  $\\mathbb{E}_{\\tilde{w}_{tj} \\sim \\mathbb{P}[w_j|w_t]}$ we simply sample words $k$ times from distribution $\\mathbb{Q}$, $\\mathbb{P}(w_j|w_t) = \\mathbb{Q}(w_j)$.\n",
    "\n",
    "#### Additional word2vec tricks\n",
    "For the best quality at the moment of learning it is proposed to use variety of tricks:\n",
    "1. How to choose $\\mathbb{Q}$.\n",
    "2. Sampling of frequent and rare words.\n",
    "3. The sampling of window size.\n",
    "You are welcome to read about this in original papers.\n",
    "\n",
    "#### Further reading\n",
    "1. Briefly about both models [here](https://arxiv.org/pdf/1301.3781.pdf).\n",
    "2. Hierarchical softmax and negative sampling [here](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n",
    "\n",
    "#### Exercises\n",
    "1. Train skip-gram model, using negative sampling and embedding size $d$ = 256. You are free to choose size of window and number of epochs.\n",
    "2. Find top-10 nearest words again. Compare Euclidean and cosinus metric. Which is better? \n",
    "3. Find 10 nearest vector for expression v(king) - v(man) + v(women). If your model is tuned well, you find representation of word queen. \n",
    "4. How could you solve the following problems?\n",
    "  * Find for country its capital\n",
    "  * For a word find its plural form\n",
    "  * For word define antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### GloVe\n",
    "This model combines two ideas:\n",
    "1. Factorization of the matrix, as in LSA\n",
    "2. Context of word is adjacent words in the corpus, just in word2vec.\n",
    "\n",
    "Let matrix $X$ at position $i,j$ stores how many times the word $j$ was found in the context of word $i$. This matrix is filled with our corpus of text. Then the probability that the word $j$ appears in the context of word $i$ is equal to\n",
    "$$P_{ij} = \\frac{X_{ij}}{\\sum_k X_{ik}}$$\n",
    "\n",
    "At the same time we want to find some function $F$ and word embeddings that \n",
    "$$F((w_i - w_j)^T w_k) = \\frac{F(w_i^T w_k)}{F(w_j^T w_k)} = \\frac{P_{ik}}{P_{jk}}.$$\n",
    "About motivation it's better to read in the original [paper](http://nlp.stanford.edu/pubs/glove.pdf).\n",
    "Possible solution may be following\n",
    "$$\\exp(w_i^T w_j) = P_{ij}.$$\n",
    "\n",
    "After a series of transformations you can receive that\n",
    "$$w_i^T w_j = \\log X_{ij} - \\log\\big(\\sum_k X_{ik}\\big)$$\n",
    "\n",
    "Obviously, right part doesn't depend on $j$, but the resulting model is offered to be written symmetry as\n",
    "$$w_i^T w_j + b_i + b_j = \\log X_{ij}$$\n",
    "\n",
    "As loss function authors suggest to use weighted MSE\n",
    "$$\\mathcal{L} = \\sum_{i,j = 1}^{|W|} f(X_{ij}) \\Big(w_i^T w_j + b_i + b_j - \\log(X_{ij}) \\Big)^2,$$\n",
    "where $f$ is some weight function, defined for each pair of words. It is reasonable to store $X$ as sparse matrix. It is also noteworthy that during training only **not null** values of $X$ used. Model is trained with stochastic gradient descent. As some way of regularization the authors suggest to use two different matrices $W$ и $W'$ for word and context, so model can be rewritten as\n",
    "$$w_i^T {w'}_j + b_i + b_j = \\log X_{ij}$$\n",
    "The resulted embedding of a word is the sum of two trained views $w + w'$.\n",
    "\n",
    "#### Some additional GloVe tricks\n",
    "1. In order to understand how you can choose $f$, please, read the original paper.\n",
    "2. Also you are welcome to see, how $X$ is formed in the original article.\n",
    "\n",
    "#### Further reading\n",
    "1. GloVe [site](http://nlp.stanford.edu/projects/glove/).\n",
    "2. Some words about [t-SNE](http://lvdmaaten.github.io/tsne/).\n",
    "\n",
    "#### Exercises\n",
    "1. Estimate complexity of model for one iteration. Choose appropriate $c$ for you.\n",
    "2. Train GloVe word embeddings ($d$=256).\n",
    "3. Check that v(king) - v(man) + v(women) is approximately equal v(queen).\n",
    "4. Read about [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding).\n",
    "5. Use t-SNE to reduce the size of embeddings to 3. Make sure that the following groups of vectors are collinear (use visualization)\n",
    "  * [man, woman], [Mr., Ms], [king, queen], etc\n",
    "  * [CEO, company]\n",
    "  * [adjective, its comparative form]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText\n",
    "[FastText](https://arxiv.org/pdf/1607.04606.pdf) is a logical development of the word2vec skip-gram model. The core feature is to present the word as the sum of its n-grams embeddings and its own embedding vector. For example, if we want to use only 3-grams, than word **fast** be reprented as n-grams **-fa**, **fas**, **ast**, **st-** and word **-fast-**. The context is encoeded in usual way, as a result similarity of word and context word is the following sum\n",
    "$$s(w, c) = \\sum_{n \\in N_w} z^{T}_n v'_{c},$$\n",
    "where $N_w$ is the set of all n-grams of word $w$ combined with word itself. The authors argue that the use of combination of 3-,4- and 5-grams helps to get better embeddings for rare and even unknown words (emdedding of word itself is null vector). For model training is proposed to use negative sampling.\n",
    "\n",
    "You can find more information on [site](https://fasttext.cc) of FastText.\n",
    "\n",
    "#### Exercises\n",
    "1. Train FastText word embeddings ($d$=256).\n",
    "2. Find some rare words in your corpus and find top-5 nearest words for them, using cosinus as metric.\n",
    "3. Compare results with classic word2vec.\n",
    "4. Invent some funny new word and found its top-5 nearest words again.\n",
    "5. How could you compare all models. Suggest your metric (see papers for inspiration)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
